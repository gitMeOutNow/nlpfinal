{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"get_outlier_data.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPO4CtQmRPm8p1CcbVH/Fy0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0l5KCNyR8jzu","executionInfo":{"status":"ok","timestamp":1651299666401,"user_tz":300,"elapsed":16881,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}},"outputId":"1bab9318-ed9b-4080-aff1-4c70da0b6870"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["# mounting drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["import json\n","import pprint\n","import os\n","import nltk\n","import pandas as pd\n","import random\n","from nltk.tokenize import RegexpTokenizer\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6MJUqYSC84LP","executionInfo":{"status":"ok","timestamp":1651299669004,"user_tz":300,"elapsed":2605,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}},"outputId":"b18f25d2-c61b-4477-8c29-88558a92e136"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["TOPIC = 'sociology'\n","METADATA_DIRECTORY = f'/content/gdrive/MyDrive/data/metadata/{TOPIC}'\n","\n","outlier_data = {\"s0\": [], \n","                \"s1\": [], \n","                \"s2\": [], \n","                \"s3\": [],\n","                \"outlier\": []}\n","\n","for file in os.listdir(METADATA_DIRECTORY):\n","  print(f'adding file {file}')\n","  metadata_path = METADATA_DIRECTORY + f'/{file}'  \n","  for i, line in enumerate(open(metadata_path, 'r')):\n","    doc = json.loads(line)\n","    title = doc['title'].lower()\n","    abstract = doc['abstract']\n","    if abstract == None:\n","      continue\n","    \n","    tokenizer = RegexpTokenizer(r'\\w+')\n","    sentences = nltk.tokenize.sent_tokenize(abstract)\n","    sentences = [\" \".join(tokenizer.tokenize(sentence.lower())) for sentence in sentences]\n","    sentences = [s + '\\n' for s in sentences]\n","\n","    if len(sentences) >= 3:\n","      split_len_0 = len(sentences[0].split(\" \"))\n","      split_len_1 = len(sentences[1].split(\" \"))\n","      split_len_2 = len(sentences[2].split(\" \"))\n","      if (split_len_0 > 5 and split_len_0 < 35 \n","          and split_len_1 > 5 and split_len_1 < 35 \n","          and split_len_2 > 5 and split_len_2 < 35):\n","        outlier_data[\"s0\"].append(sentences[0])\n","        outlier_data[\"s1\"].append(sentences[1])\n","        outlier_data[\"s2\"].append(sentences[2])\n","\n","    if i > 15000:\n","      break\n","\n","# get outliers\n","for i in range(len(outlier_data['s0'])):\n","  random_idx = random.randint(1,3)\n","  random_sentence = None\n","  if random_idx == 1:\n","    random_sentence = random.choice(outlier_data['s0'])\n","  elif random_idx == 2:\n","    random_sentence = random.choice(outlier_data['s1'])\n","  elif random_idx == 3:\n","    random_sentence = random.choice(outlier_data['s2'])\n","\n","  outlier_data['s3'].append(random_sentence)\n","\n","  # randomly swap sentences\n","  outlier_class = random.randint(0,3)\n","  swap_sentence = None\n","  if outlier_class == 0:\n","    temp = outlier_data['s0'][i]\n","    outlier_data['s0'][i] = outlier_data['s3'][i]\n","    outlier_data['s3'][i] = temp\n","  elif outlier_class == 1:\n","    temp = outlier_data['s1'][i]\n","    outlier_data['s1'][i] = outlier_data['s3'][i]\n","    outlier_data['s3'][i] = temp\n","  elif outlier_class == 2:\n","    temp = outlier_data['s2'][i]\n","    outlier_data['s2'][i] = outlier_data['s3'][i]\n","    outlier_data['s3'][i] = temp\n","\n","  outlier_data['outlier'].append(outlier_class)\n","\n","df = pd.DataFrame(outlier_data)\n","# print(df.shape)\n","df.to_csv(f'/content/gdrive/MyDrive/data/eval_data/outlier_detection/{TOPIC}/{TOPIC}.tsv', sep = \" \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5QNgNUad88oq","executionInfo":{"status":"ok","timestamp":1651300260073,"user_tz":300,"elapsed":109926,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}},"outputId":"9d3eb9c2-4c7b-496b-e092-61e4d206ce6b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["adding file metadata_0.jsonl\n","adding file metadata_1.jsonl\n","adding file metadata_10.jsonl\n","adding file metadata_11.jsonl\n","adding file metadata_12.jsonl\n","adding file metadata_13.jsonl\n","adding file metadata_14.jsonl\n","adding file metadata_15.jsonl\n","adding file metadata_16.jsonl\n","adding file metadata_17.jsonl\n","adding file metadata_18.jsonl\n","adding file metadata_19.jsonl\n","adding file metadata_2.jsonl\n","adding file metadata_20.jsonl\n","adding file metadata_21.jsonl\n","adding file metadata_22.jsonl\n","adding file metadata_23.jsonl\n","adding file metadata_24.jsonl\n","adding file metadata_25.jsonl\n","adding file metadata_26.jsonl\n","adding file metadata_27.jsonl\n","adding file metadata_28.jsonl\n","adding file metadata_29.jsonl\n","adding file metadata_3.jsonl\n","adding file metadata_30.jsonl\n","adding file metadata_31.jsonl\n","adding file metadata_32.jsonl\n","adding file metadata_33.jsonl\n","adding file metadata_34.jsonl\n","adding file metadata_35.jsonl\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"GbE8SQY8_lNT","executionInfo":{"status":"ok","timestamp":1651299853251,"user_tz":300,"elapsed":8,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}}},"execution_count":3,"outputs":[]}]}