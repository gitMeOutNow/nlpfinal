{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"outlier_detection.ipynb","provenance":[{"file_id":"1juAb2F3Q7rZAitWKdgvonZk2ySfzYjYb","timestamp":1650658852148}],"collapsed_sections":[],"authorship_tag":"ABX9TyMVd8YcZKjFXO62WNOQxbwF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-RtcAZw7N0_K","executionInfo":{"status":"ok","timestamp":1651940345123,"user_tz":300,"elapsed":18135,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}},"outputId":"75989330-90f7-486c-c6d8-4fea2dcdb073"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["# mounting drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""],"metadata":{"id":"n7Lhv5b8vzcW","executionInfo":{"status":"ok","timestamp":1651940345125,"user_tz":300,"elapsed":9,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from torch import optim\n","import random\n","from copy import deepcopy\n","import time\n","import pprint\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')"],"metadata":{"id":"_YRfHVb4OTes","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651940352615,"user_tz":300,"elapsed":7497,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}},"outputId":"275d5501-60c7-4f45-96d6-f3d30618e621"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["print(torch.cuda.is_available())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oxFqC_DE7vEn","executionInfo":{"status":"ok","timestamp":1651940352778,"user_tz":300,"elapsed":5,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}},"outputId":"04caf896-edc1-4e15-aded-8268798bda6d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n"]}]},{"cell_type":"code","source":["TOPIC = 'polisci'\n","DIMENSIONS = 200"],"metadata":{"id":"q-kpQ6S68hvQ","executionInfo":{"status":"ok","timestamp":1651940352779,"user_tz":300,"elapsed":4,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# read in classification dataset and store in dataframe\n","df = pd.read_csv(f'/content/gdrive/MyDrive/data/eval_data/outlier_detection/{TOPIC}/{TOPIC}.tsv', sep=' ', index_col=0)"],"metadata":{"id":"-2dLSrv0OT_F","executionInfo":{"status":"ok","timestamp":1651940354474,"user_tz":300,"elapsed":1698,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["df.head(n=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"ryAPtH2KOfOH","executionInfo":{"status":"ok","timestamp":1651940354478,"user_tz":300,"elapsed":14,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}},"outputId":"6036004a-505c-4f53-9da1-162c529bbaef"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                  s0  \\\n","0    the overall aim of this project was threefold\\n   \n","1  older australians lack guidance from and recou...   \n","2  the government of india has presented an expan...   \n","3  it is made the quantitative assessment of the ...   \n","4  report from the norwegian scientific committee...   \n","5  in july 2016 the law on temporary limitations ...   \n","6  elite analysis has presented a major challenge...   \n","7  since the 1980s globalization has led to a new...   \n","8  trends in female migration in latin america ar...   \n","9  a movement has ignited amongst government agen...   \n","\n","                                                  s1  \\\n","0  by organizing workshops at the science policy ...   \n","1  consequently they lack identity as rights hold...   \n","2  the key to its success say dozens of people wh...   \n","3  its visibility grew significantly arousing spo...   \n","4  microplastics occurrence levels and implicatio...   \n","5  the temporary law implied a sharp turn in swed...   \n","6  it was a period of enlightenment marked by int...   \n","7  studying american politics in america s century\\n   \n","8  consideration is given to the causes of this m...   \n","9  these projects face challenges related to scal...   \n","\n","                                                  s2  \\\n","0  raise awareness for climate change in turkish ...   \n","1  thank you for downloading social forces in the...   \n","2  the awc has treatment facilities in the surges...   \n","3  from a sample of mails sent to the judges of t...   \n","4  opinion of the steering committee of the norwe...   \n","5  that such a large segment of ireland s populat...   \n","6  it is natural that one of the many subjects th...   \n","7  today globalization is accepted as an umbrella...   \n","8  data are from a survey of migrant origin carri...   \n","9  but new sociotechnical approaches that leverag...   \n","\n","                                                  s3  outlier  \n","0                a survey research design was used\\n        3  \n","1  accordingly as individuals they are vulnerable...        2  \n","2  taking this vision forward the rashtrapati bha...        1  \n","3  the constitutional council conquered during th...        0  \n","4  a political approach the organization for secu...        3  \n","5  the right to family reunification which previo...        2  \n","6  women s movement in india emerged as a part of...        0  \n","7  it has exhibited a monolithic order on the one...        1  \n","8  in this age of electronic information we live ...        3  \n","9  we then show how the failure of this initial r...        3  "],"text/html":["\n","  <div id=\"df-6a3653df-2eaa-44cb-9e75-15bd52044381\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>s0</th>\n","      <th>s1</th>\n","      <th>s2</th>\n","      <th>s3</th>\n","      <th>outlier</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>the overall aim of this project was threefold\\n</td>\n","      <td>by organizing workshops at the science policy ...</td>\n","      <td>raise awareness for climate change in turkish ...</td>\n","      <td>a survey research design was used\\n</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>older australians lack guidance from and recou...</td>\n","      <td>consequently they lack identity as rights hold...</td>\n","      <td>thank you for downloading social forces in the...</td>\n","      <td>accordingly as individuals they are vulnerable...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>the government of india has presented an expan...</td>\n","      <td>the key to its success say dozens of people wh...</td>\n","      <td>the awc has treatment facilities in the surges...</td>\n","      <td>taking this vision forward the rashtrapati bha...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>it is made the quantitative assessment of the ...</td>\n","      <td>its visibility grew significantly arousing spo...</td>\n","      <td>from a sample of mails sent to the judges of t...</td>\n","      <td>the constitutional council conquered during th...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>report from the norwegian scientific committee...</td>\n","      <td>microplastics occurrence levels and implicatio...</td>\n","      <td>opinion of the steering committee of the norwe...</td>\n","      <td>a political approach the organization for secu...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>in july 2016 the law on temporary limitations ...</td>\n","      <td>the temporary law implied a sharp turn in swed...</td>\n","      <td>that such a large segment of ireland s populat...</td>\n","      <td>the right to family reunification which previo...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>elite analysis has presented a major challenge...</td>\n","      <td>it was a period of enlightenment marked by int...</td>\n","      <td>it is natural that one of the many subjects th...</td>\n","      <td>women s movement in india emerged as a part of...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>since the 1980s globalization has led to a new...</td>\n","      <td>studying american politics in america s century\\n</td>\n","      <td>today globalization is accepted as an umbrella...</td>\n","      <td>it has exhibited a monolithic order on the one...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>trends in female migration in latin america ar...</td>\n","      <td>consideration is given to the causes of this m...</td>\n","      <td>data are from a survey of migrant origin carri...</td>\n","      <td>in this age of electronic information we live ...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>a movement has ignited amongst government agen...</td>\n","      <td>these projects face challenges related to scal...</td>\n","      <td>but new sociotechnical approaches that leverag...</td>\n","      <td>we then show how the failure of this initial r...</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a3653df-2eaa-44cb-9e75-15bd52044381')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6a3653df-2eaa-44cb-9e75-15bd52044381 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6a3653df-2eaa-44cb-9e75-15bd52044381');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["train_percent = 0.8\n","validation_percent = 0.04\n","test_percent = 0.04\n","\n","df = df.sample(frac=1)\n","a = int(len(df)*train_percent)\n","b = int(a + len(df)*validation_percent)\n","c = int(b + len(df)*test_percent)\n","\n","train_data = df[0:a]\n","validation_data = df[a:b]\n","test_data = df[b:c]\n","\n","print(train_data.head())\n","print(validation_data.head())\n","print(test_data.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fDKMmp8mOfI9","executionInfo":{"status":"ok","timestamp":1651940354481,"user_tz":300,"elapsed":13,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}},"outputId":"9d366ccc-c7f8-4757-a23c-180eccba874d"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                      s0  \\\n","36444  this is our introduction to the forthcoming bo...   \n","42291  the mission of wageningen ur university resear...   \n","35545  in this research a set of questionnaires consi...   \n","24531  strategic communication disciplines routinely ...   \n","52282  the development of agricultural engineering di...   \n","\n","                                                      s1  \\\n","36444  in the introduction we explain the aim and met...   \n","42291  within wageningen ur nine specialised research...   \n","35545  it highlights the place of the hungarian refor...   \n","24531  i argue here that a better understanding of cl...   \n","52282  it concerns the complexity of migration moveme...   \n","\n","                                                      s2  \\\n","36444  the researcher finds out the awareness level o...   \n","42291  because of the importance of international stu...   \n","35545  calvinism attracted strong support in hungary ...   \n","24531  as u s involvement in the wars of afghanistan ...   \n","52282  the development course of chinese agricultural...   \n","\n","                                                      s3  outlier  \n","36444  in particular we defend the common core method...        2  \n","42291  with approximately 30 locations 6 000 members ...        2  \n","35545  this is the first book to examine one of europ...        0  \n","24531  case studies of unscrupulous public relations ...        2  \n","52282  great change has taken place in this disciplin...        1  \n","                                                      s0  \\\n","13100  abstract how should the united states determin...   \n","43655  this paper is part of the outcomes of the rese...   \n","11704  the author discusses the relationship between ...   \n","37233  normative legal research methods or library re...   \n","7390   methodological pluralism advocates balanced co...   \n","\n","                                                      s1  \\\n","13100  in the next and also the main part of this wor...   \n","43655  however what is the impact of all these parent...   \n","11704  included are such questions as what is right i...   \n","37233  these decisions have become increasingly more ...   \n","7390   surely if england has developed such a system ...   \n","\n","                                                      s2  \\\n","13100  current policy is designed to deter chemical a...   \n","43655  this topic deeply recognised as a strategic as...   \n","11704  what is justifiable in the reaction against th...   \n","37233  the concept of shared or distributive leadersh...   \n","7390   ecological economics however appears to be eng...   \n","\n","                                                      s3  outlier  \n","13100  this paper assesses the current u s reprisal p...        1  \n","43655  paper s objective is to analyse genoa area by ...        1  \n","11704  some commentators argue that the current syste...        3  \n","37233  a common thread in contemporary research on pr...        0  \n","7390   the concept rests upon the necessity of choice...        1  \n","                                                      s0  \\\n","22351  in every society in many arenas the reality of...   \n","28835       global population and demographic trends 3\\n   \n","24868  non standard employment is becoming more common\\n   \n","1685   how do natural disasters affect aspirations an...   \n","37962  a community engages in multiple authority dele...   \n","\n","                                                      s1  \\\n","22351  radially extending portions of the flow passag...   \n","28835  it gives them legal rights or duties which ens...   \n","24868  fewer people are working full time and or have...   \n","1685   design methodology approach the coverage organ...   \n","37962  it is specially emphasized in the urban planni...   \n","\n","                                                      s2  \\\n","22351  part of the solutions to these challenges lies...   \n","28835  the issue of a state as a subject of law has n...   \n","24868  these models provide school board and school a...   \n","1685   findings these two political reference handboo...   \n","37962  this article introduces the working stages and...   \n","\n","                                                      s3  outlier  \n","22351  these shortfalls include disenfranchisement un...        1  \n","28835  financial law regulates a special part of soci...        0  \n","24868  many scholars have pointed to the negative con...        2  \n","1685   purpose the purpose of this article is to revi...        0  \n","37962  as an important tool which is determined by th...        0  \n"]}]},{"cell_type":"code","source":["class SentenceExample:\n","    \"\"\"\n","    Data wrapper for a single example for sentiment analysis.\n","\n","    Attributes:\n","        words (List[string]): list of words\n","        label (int): 0 or 1 (0 = negative, 1 = positive)\n","    \"\"\"\n","\n","    def __init__(self, s1, s2, s3, s4, outlier):\n","        self.s1 = s1\n","        self.s2 = s2\n","        self.s3 = s3\n","        self.s4 = s4\n","        self.outlier = outlier\n","\n","    def __repr__(self):\n","        return f'outlier = {self.outlier}'\n","\n","    def __str__(self):\n","        return self.__repr__()"],"metadata":{"id":"uzpkMHNgOteE","executionInfo":{"status":"ok","timestamp":1651940354643,"user_tz":300,"elapsed":8,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class Indexer(object):\n","    \"\"\"\n","    Bijection between objects and integers starting at 0. Useful for mapping\n","    labels, features, etc. into coordinates of a vector space.\n","\n","    Attributes:\n","        objs_to_ints\n","        ints_to_objs\n","    \"\"\"\n","    def __init__(self):\n","        self.objs_to_ints = {}\n","        self.ints_to_objs = {}\n","\n","    def __repr__(self):\n","        return str([str(self.get_object(i)) for i in range(0, len(self))])\n","\n","    def __str__(self):\n","        return self.__repr__()\n","\n","    def __len__(self):\n","        return len(self.objs_to_ints)\n","\n","    def get_object(self, index):\n","        \"\"\"\n","        :param index: integer index to look up\n","        :return: Returns the object corresponding to the particular index or None if not found\n","        \"\"\"\n","        if (index not in self.ints_to_objs):\n","            return None\n","        else:\n","            return self.ints_to_objs[index]\n","\n","    def contains(self, object):\n","        \"\"\"\n","        :param object: object to look up\n","        :return: Returns True if it is in the Indexer, False otherwise\n","        \"\"\"\n","        return self.index_of(object) != -1\n","\n","    def index_of(self, object):\n","        \"\"\"\n","        :param object: object to look up\n","        :return: Returns -1 if the object isn't present, index otherwise\n","        \"\"\"\n","        if (object not in self.objs_to_ints):\n","            return -1\n","        else:\n","            return self.objs_to_ints[object]\n","\n","    def add_and_get_index(self, object, add=True):\n","        \"\"\"\n","        Adds the object to the index if it isn't present, always returns a nonnegative index\n","        :param object: object to look up or add\n","        :param add: True by default, False if we shouldn't add the object. If False, equivalent to index_of.\n","        :return: The index of the object\n","        \"\"\"\n","        if not add:\n","            return self.index_of(object)\n","        if (object not in self.objs_to_ints):\n","            new_idx = len(self.objs_to_ints)\n","            self.objs_to_ints[object] = new_idx\n","            self.ints_to_objs[new_idx] = object\n","        return self.objs_to_ints[object]"],"metadata":{"id":"vVFCmhUbCyhE","executionInfo":{"status":"ok","timestamp":1651940354643,"user_tz":300,"elapsed":7,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class WordEmbeddings:\n","    \"\"\"\n","    Wraps an Indexer and a list of 1-D numpy arrays where each position in the list is the vector for the corresponding\n","    word in the indexer. The 0 vector is returned if an unknown word is queried.\n","    \"\"\"\n","    def __init__(self, word_indexer, vectors):\n","        self.word_indexer = word_indexer\n","        self.vectors = vectors\n","\n","    def get_embedding_length(self):\n","        return len(self.vectors[0])\n","\n","    def get_embedding(self, word):\n","        \"\"\"\n","        Returns the embedding for a given word\n","        :param word: The word to look up\n","        :return: The UNK vector if the word is not in the Indexer or the vector otherwise\n","        \"\"\"\n","        word_idx = self.word_indexer.index_of(word)\n","        if word_idx != -1:\n","            return self.vectors[word_idx]\n","        else:\n","            return self.vectors[self.word_indexer.index_of(\"UNK\")]"],"metadata":{"id":"RXD4qyHOPGUu","executionInfo":{"status":"ok","timestamp":1651940354643,"user_tz":300,"elapsed":7,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["CUSTOM_EMBEDDINGS = False"],"metadata":{"id":"8j9z3dmx1PGK","executionInfo":{"status":"ok","timestamp":1651940354644,"user_tz":300,"elapsed":7,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def read_word_embeddings(embeddings_file: str) -> WordEmbeddings:\n","    \"\"\"\n","    Loads the given embeddings (ASCII-formatted) into a WordEmbeddings object. Augments this with an UNK embedding\n","    that is the 0 vector. Reads in all embeddings with no filtering -- you should only use this for relativized\n","    word embedding files.\n","    :param embeddings_file: path to the file containing embeddings\n","    :return: WordEmbeddings object reflecting the words and their embeddings\n","    \"\"\"\n","    f = open(embeddings_file)\n","    word_indexer = Indexer()\n","    vectors = []\n","    # Make position 0 a PAD token, which can be useful if you\n","    word_indexer.add_and_get_index(\"PAD\")\n","    # Make position 1 the UNK token\n","    word_indexer.add_and_get_index(\"UNK\")\n","    for i, line in enumerate(f):\n","        if CUSTOM_EMBEDDINGS and i == 0:\n","          continue\n","\n","        if line.strip() != \"\":\n","            space_idx = line.find(' ')\n","            word = line[:space_idx]\n","            numbers = line[space_idx+1:]\n","            float_numbers = [float(number_str) for number_str in numbers.split()]\n","            vector = np.array(float_numbers)\n","            word_indexer.add_and_get_index(word)\n","            # Append the PAD and UNK vectors to start. Have to do this weirdly because we need to read the first line\n","            # of the file to see what the embedding dim is\n","            if len(vectors) == 0:\n","                vectors.append(np.zeros(vector.shape[0]))\n","                vectors.append(np.zeros(vector.shape[0]))\n","            vectors.append(vector)\n","        if i % 50000 == 0:\n","          print(f'done reading {i} embeddings')\n","    f.close()\n","    print(\"Read in \" + repr(len(word_indexer)) + \" vectors of size \" + repr(vectors[0].shape[0]))\n","    # Turn vectors into a 2-D numpy array\n","    return WordEmbeddings(word_indexer, np.array(vectors))"],"metadata":{"id":"eZuwuTwKFgtu","executionInfo":{"status":"ok","timestamp":1651940354646,"user_tz":300,"elapsed":9,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["embeddings = read_word_embeddings(f\"/content/gdrive/MyDrive/embeddings/glove/baseline/glove.6B.{DIMENSIONS}d.txt\")\n","#embeddings = read_word_embeddings(f\"/content/gdrive/MyDrive/embeddings/word2vec/baseline/word2vec_embeddings_{TOPIC}.txt\")\n","print(len(embeddings.word_indexer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cZhPMzXEFkRb","outputId":"9e785e1e-385d-4af2-cd71-bf622b00f1a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["done reading 0 embeddings\n","done reading 50000 embeddings\n","done reading 100000 embeddings\n"]}]},{"cell_type":"code","source":["print(embeddings.word_indexer.index_of('super'))"],"metadata":{"id":"N3CCley7PUEK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_exs = []\n","for i, row in train_data.iterrows():\n","\n","  s1 = word_tokenize(row['s0'])\n","  s1[-1] = s1[-1].strip()\n","\n","  s2 = word_tokenize(row['s1'])\n","  s2[-1] = s2[-1].strip()\n","\n","  s3 = word_tokenize(row['s2'])\n","  s3[-1] = s3[-1].strip()\n","\n","  s4 = word_tokenize(row['s3'])\n","  s4[-1] = s4[-1].strip()\n","  \n","  train_exs.append(SentenceExample(s1, s2, s3, s4, row['outlier']))"],"metadata":{"id":"0KpxxnLMOyia"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(train_exs))"],"metadata":{"id":"0PRyAWtiXFZ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["validation_exs = []\n","for i, row in validation_data.iterrows():\n","  s1 = word_tokenize(row['s0'])\n","  s1[-1] = s1[-1].strip()\n","\n","  s2 = word_tokenize(row['s1'])\n","  s2[-1] = s2[-1].strip()\n","\n","  s3 = word_tokenize(row['s2'])\n","  s3[-1] = s3[-1].strip()\n","\n","  s4 = word_tokenize(row['s3'])\n","  s4[-1] = s4[-1].strip()\n","  \n","  validation_exs.append(SentenceExample(s1, s2, s3, s4, row['outlier']))"],"metadata":{"id":"LmGtBe5XOzF4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_exs = []\n","for i, row in test_data.iterrows():\n","  s1 = word_tokenize(row['s0'])\n","  s1[-1] = s1[-1].strip()\n","\n","  s2 = word_tokenize(row['s1'])\n","  s2[-1] = s2[-1].strip()\n","\n","  s3 = word_tokenize(row['s2'])\n","  s3[-1] = s3[-1].strip()\n","\n","  s4 = word_tokenize(row['s3'])\n","  s4[-1] = s4[-1].strip()\n","  \n","  test_exs.append(SentenceExample(s1, s2, s3, s4, row['outlier']))"],"metadata":{"id":"GRD0RcCnO0yI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RNN(nn.Module):\n","  def __init__(self, \n","               embedding_vectors,\n","               hidden_dim,\n","               output_dim):\n","    super().__init__()\n","\n","    self.embedding_vectors = embedding_vectors\n","    self.hidden_dim = hidden_dim\n","    self.output_dim = output_dim\n","\n","    weights = torch.FloatTensor(self.embedding_vectors)\n","    self.e1 = nn.Embedding.from_pretrained(weights, padding_idx=0)\n","    self.e1.requires_grad_(False)\n","\n","    self.gru_s1 = nn.GRU(input_size=len(self.embedding_vectors[0]),\n","                         hidden_size=self.hidden_dim,\n","                         num_layers=2,\n","                         batch_first=True,\n","                         bidirectional=False)\n","    \n","    self.gru_s2 = nn.GRU(input_size=len(self.embedding_vectors[0]),\n","                         hidden_size=self.hidden_dim,\n","                         num_layers=2,\n","                         batch_first=True,\n","                         bidirectional=False)\n","    \n","    self.gru_s3 = nn.GRU(input_size=len(self.embedding_vectors[0]),\n","                         hidden_size=self.hidden_dim,\n","                         num_layers=2,\n","                         batch_first=True,\n","                         bidirectional=False)\n","    \n","    self.gru_s4 = nn.GRU(input_size=len(self.embedding_vectors[0]),\n","                         hidden_size=self.hidden_dim,\n","                         num_layers=2,\n","                         batch_first=True,\n","                         bidirectional=False)\n","    \n","    self.drop = nn.Dropout(p=0.3)\n","\n","    self.l1 = nn.Linear(4*self.hidden_dim, self.hidden_dim)\n","    self.r1 = nn.ReLU()\n","    self.l2 = nn.Linear(self.hidden_dim, self.output_dim)\n","\n","    nn.init.xavier_uniform_(self.l1.weight)\n","    nn.init.xavier_uniform_(self.l2.weight)\n","\n","  def forward(self, s1, s2, s3, s4):\n","\n","    s1_mask = (s1 != 0).type(\n","            torch.cuda.LongTensor if s1.is_cuda else\n","            torch.LongTensor)\n","    \n","    s1_lengths = s1_mask.sum(dim=1).cpu()\n","\n","    s2_mask = (s2 != 0).type(\n","            torch.cuda.LongTensor if s2.is_cuda else\n","            torch.LongTensor)\n","    \n","    s2_lengths = s2_mask.sum(dim=1).cpu()\n","    \n","    s3_mask = (s3 != 0).type(\n","            torch.cuda.LongTensor if s3.is_cuda else\n","            torch.LongTensor)\n","    \n","    s3_lengths = s3_mask.sum(dim=1).cpu()\n","\n","    s4_mask = (s4 != 0).type(\n","            torch.cuda.LongTensor if s4.is_cuda else\n","            torch.LongTensor)\n","    \n","    s4_lengths = s4_mask.sum(dim=1).cpu()\n","            \n","    s1 = self.e1(s1)\n","    s2 = self.e1(s2)\n","    s3 = self.e1(s3)\n","    s4 = self.e1(s4)\n","\n","    packed_input_s1 = pack_padded_sequence(s1, s1_lengths, batch_first=True, enforce_sorted=False)\n","    packed_output_s1, _ = self.gru_s1(packed_input_s1)\n","    output_s1, _ = pad_packed_sequence(packed_output_s1, batch_first=True)\n","    out_s1 = output_s1[range(len(output_s1)), s1_lengths - 1, :self.hidden_dim]\n","    text_fea_s1 = self.drop(out_s1)\n","\n","    packed_input_s2 = pack_padded_sequence(s2, s2_lengths, batch_first=True, enforce_sorted=False)\n","    packed_output_s2, _ = self.gru_s2(packed_input_s2)\n","    output_s2, _ = pad_packed_sequence(packed_output_s2, batch_first=True)\n","    out_s2 = output_s2[range(len(output_s2)), s2_lengths - 1, :self.hidden_dim]\n","    text_fea_s2 = self.drop(out_s2)\n","\n","    packed_input_s3 = pack_padded_sequence(s3, s3_lengths, batch_first=True, enforce_sorted=False)\n","    packed_output_s3, _ = self.gru_s3(packed_input_s3)\n","    output_s3, _ = pad_packed_sequence(packed_output_s3, batch_first=True)\n","    out_s3 = output_s3[range(len(output_s3)), s3_lengths - 1, :self.hidden_dim]\n","    text_fea_s3 = self.drop(out_s3)\n","\n","    packed_input_s4 = pack_padded_sequence(s4, s4_lengths, batch_first=True, enforce_sorted=False)\n","    packed_output_s4, _ = self.gru_s4(packed_input_s4)\n","    output_s4, _ = pad_packed_sequence(packed_output_s4, batch_first=True)\n","    out_s4 = output_s4[range(len(output_s4)), s4_lengths - 1, :self.hidden_dim]\n","    text_fea_s4 = self.drop(out_s4)\n","\n","    input_fea = torch.cat((text_fea_s1, text_fea_s2, text_fea_s3, text_fea_s4), dim=1)\n","\n","    x = self.l1(input_fea)\n","    x = self.r1(x)\n","    x = self.l2(x)\n","\n","    return x"],"metadata":{"id":"uFNvilwdPXIO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RNNClassifier():\n","    \"\"\"\n","    Implement your NeuralSentimentClassifier here. This should wrap an instance of the network with learned weights\n","    along with everything needed to run it on new data (word embeddings, etc.)\n","    \"\"\"\n","    def __init__(self,\n","                 word_embeddings,\n","                 batch_size=256,\n","                 hidden_size=128,\n","                 output_size=5,\n","                 lr=0.001,\n","                 num_epochs=10,\n","                 seed=3):\n","\n","        # indexer between words and indexes => self.word_embeddings.word_indexer (indexer in utils)\n","        # 2D array of weights => self.word_embeddings.vectors (in sentiment_data)\n","        \n","        self.word_embeddings = word_embeddings\n","        self.batch_size = batch_size\n","        self.num_epochs = num_epochs\n","        self.lr = lr\n","        self.output_size = output_size\n","\n","        # random.seed(seed)\n","        # torch.manual_seed(seed)\n","\n","        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","        self.network = RNN(embedding_vectors=self.word_embeddings.vectors,\n","                           hidden_dim=hidden_size,\n","                           output_dim=output_size).to(self.device)\n","\n","        self.print_network()\n","\n","    def convert_to_idx_tensor(self, sentences):\n","        idx_encodings = []\n","        for sentence in sentences:\n","            idx_encoding = [0] * len(sentence)\n","            for idx, word in enumerate(sentence):\n","                word_idx = self.word_embeddings.word_indexer.index_of(word)\n","                idx_encoding[idx] = word_idx if word_idx != -1 else 1\n","            idx_encodings.append(idx_encoding)\n","        idx_encodings = torch.tensor(idx_encodings)\n","        return idx_encodings\n","\n","    def pad_sentences(self, batch):\n","        max_len = 0\n","        for sentiment_ex in batch:\n","            sentence_s1 = sentiment_ex.s1\n","            max_len = max(max_len, len(sentence_s1))\n","        for sentiment_ex in batch:\n","            sentence_s1 = sentiment_ex.s1\n","            sentence_s1 += ['PAD'] * (max_len - len(sentence_s1))\n","\n","        max_len = 0\n","        for sentiment_ex in batch:\n","            sentence_s2 = sentiment_ex.s2\n","            max_len = max(max_len, len(sentence_s2))\n","        for sentiment_ex in batch:\n","            sentence_s2 = sentiment_ex.s2\n","            sentence_s2 += ['PAD'] * (max_len - len(sentence_s2))\n","\n","        max_len = 0\n","        for sentiment_ex in batch:\n","            sentence_s3 = sentiment_ex.s3\n","            max_len = max(max_len, len(sentence_s3))\n","        for sentiment_ex in batch:\n","            sentence_s3 = sentiment_ex.s3\n","            sentence_s3 += ['PAD'] * (max_len - len(sentence_s3))\n","\n","        max_len = 0\n","        for sentiment_ex in batch:\n","            sentence_s4 = sentiment_ex.s4\n","            max_len = max(max_len, len(sentence_s4))\n","        for sentiment_ex in batch:\n","            sentence_s4 = sentiment_ex.s4\n","            sentence_s4 += ['PAD'] * (max_len - len(sentence_s4))\n","\n","        return batch\n","\n","    def train(self, train_exs, dev_exs):\n","        \n","        loss_function = nn.CrossEntropyLoss()\n","        optimizer = optim.Adam(self.network.parameters(), lr=self.lr, weight_decay=0.0001)\n","\n","        max_dev_acc = 0\n","        best_weights = None\n","\n","        for epoch in range(self.num_epochs):\n","\n","            self.network.train()\n","            random.shuffle(train_exs)\n","\n","            \"\"\"\n","            Minibatch stuff\n","            \"\"\"\n","            idx = 0\n","            minibatch_train_exs = []\n","            for i in range(0, len(train_exs), self.batch_size):\n","                minibatch_train_exs.append(train_exs[i:i+self.batch_size])\n","                self.pad_sentences(minibatch_train_exs[idx])\n","                idx += 1\n","\n","            total_loss = 0\n","            for i, exs in enumerate(minibatch_train_exs):\n","\n","                train_s1 = [ex.s1 for ex in exs]\n","                train_s2 = [ex.s2 for ex in exs]\n","                train_s3 = [ex.s3 for ex in exs]\n","                train_s4 = [ex.s4 for ex in exs]\n","                train_y = [ex.outlier for ex in exs]\n","\n","                # convert word sentences to list of indexes and train_y to tensor\n","                train_s1 = self.convert_to_idx_tensor(train_s1)\n","                train_s2 = self.convert_to_idx_tensor(train_s2)\n","                train_s3 = self.convert_to_idx_tensor(train_s3)\n","                train_s4 = self.convert_to_idx_tensor(train_s4)\n","                y = torch.tensor(train_y)\n","\n","                train_s1 = train_s1.to(self.device)\n","                train_s2 = train_s2.to(self.device)\n","                train_s3 = train_s3.to(self.device)\n","                train_s4 = train_s4.to(self.device)\n","                y = y.to(self.device)\n","\n","                output = self.network(train_s1, train_s2, train_s3, train_s4)\n","\n","                loss = loss_function(output, y)\n","                total_loss += loss\n","    \n","                # update model weights\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","                if i % 50 == 0:\n","                  print(f'batch {i} complete')\n","\n","            # calculate validation set accuracy\n","            validation_s1 = [ex.s1 for ex in dev_exs]\n","            validation_s2 = [ex.s2 for ex in dev_exs]\n","            validation_s3 = [ex.s3 for ex in dev_exs]\n","            validation_s4 = [ex.s4 for ex in dev_exs]\n","            golds = [ex.outlier for ex in dev_exs]\n","            predictions = self.predict_all(validation_s1, validation_s2, validation_s3, validation_s4)\n","            num_correct = 0\n","            for i in range(0, len(golds)):\n","              if golds[i] == predictions[i]:\n","                num_correct += 1\n","\n","            dev_acc = num_correct/len(golds)\n","\n","            if dev_acc > max_dev_acc or epoch == 0:\n","              max_dev_acc = dev_acc\n","              best_weights = deepcopy(self.network.state_dict())\n","              print('saving model weights...')\n","\n","            print(f'Epoch Number = {epoch}, total loss = ', total_loss)\n","            print(f'Development Accuracy = {num_correct}/{len(golds)} = {dev_acc}')\n","\n","        self.network.load_state_dict(best_weights)\n","\n","    def predict(self, q1, q2, q3, q4) -> int:\n","\n","        q1 = self.convert_to_idx_tensor([q1])\n","        q2 = self.convert_to_idx_tensor([q2])\n","        q3 = self.convert_to_idx_tensor([q3])\n","        q4 = self.convert_to_idx_tensor([q4])\n","\n","        q1 = q1.to(self.device)\n","        q2 = q2.to(self.device)\n","        q3 = q3.to(self.device)\n","        q4 = q4.to(self.device)\n","\n","        output = self.network(q1, q2, q3, q4)\n","        result = output.argmax(dim=1)\n","        return result[0]\n","\n","    def predict_all(self, s1, s2, s3, s4):\n","        \"\"\"\n","        You can leave this method with its default implementation, or you can override it to a batched version of\n","        prediction if you'd like. Since testing only happens once, this is less critical to optimize than training\n","        for the purposes of this assignment.\n","        :param all_ex_words: A list of all exs to do prediction on\n","        :return:\n","        \"\"\"\n","        return [self.predict(q1, q2, q3, q4) for q1, q2, q3, q4 in zip(s1, s2, s3, s4)]\n","\n","    def print_network(self):\n","        print(self.network)"],"metadata":{"id":"zPJchdZ9Pj4V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_deep_averaging_network(train_exs, \n","                                 dev_exs,\n","                                 word_embeddings: WordEmbeddings) -> RNNClassifier:\n","    \"\"\"\n","    :param args: Command-line args so you can access them here\n","    :param train_exs: training examples\n","    :param dev_exs: development set, in case you wish to evaluate your model during training\n","    :param word_embeddings: set of loaded word embeddings\n","    :return: A trained NeuralSentimentClassifier model\n","    \"\"\"\n","\n","    # extract input information from args\n","    batch_size = 128 # args.batch_size\n","    hidden_size = 256 # args.hidden_size\n","    lr = 0.001 # args.lr\n","    num_epochs = 5 # args.num_epochs\n","    output_size = 4\n","\n","    classifier = RNNClassifier(word_embeddings=word_embeddings,\n","                               batch_size=batch_size, \n","                               hidden_size=hidden_size, \n","                               output_size=output_size,\n","                               lr=lr, \n","                               num_epochs=num_epochs)\n","\n","    start_time = time.time()\n","    classifier.train(train_exs=train_exs, dev_exs=dev_exs)\n","    end_time = time.time()\n","    total_time = int(end_time - start_time)\n","\n","    print(f'Total time taken => {total_time}s')\n","\n","    return classifier"],"metadata":{"id":"zpC4ekZmRWRQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = train_deep_averaging_network(train_exs, validation_exs, embeddings)"],"metadata":{"id":"SNy4wvwrReUn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model.network, f'/content/gdrive/MyDrive/models/outlier_detection/outlier_detection.word2vec.{TOPIC}.{DIMENSIONS}d.pt')"],"metadata":{"id":"77dBvPP69Blu"},"execution_count":null,"outputs":[]}]}