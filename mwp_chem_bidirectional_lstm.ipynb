{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mwp_chem_bidirectional_lstm.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPfq4eSoe9/gehcUcvbEPjn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9IUv6VnrM-Mv","executionInfo":{"status":"ok","timestamp":1650608695453,"user_tz":300,"elapsed":708,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}},"outputId":"f4cb2961-53e0-4973-f9db-9fc9e7b06bf7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["# mounting drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from torch import optim\n","import random\n","from copy import deepcopy\n","import time\n","import pprint"],"metadata":{"id":"ItTdLjHOVWDP","executionInfo":{"status":"ok","timestamp":1650608701590,"user_tz":300,"elapsed":6139,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["print(torch.cuda.is_available())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cjcpzSnZVds8","executionInfo":{"status":"ok","timestamp":1650608701674,"user_tz":300,"elapsed":5,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}},"outputId":"c7775659-e6b0-428f-a3d9-892c1d738442"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"code","source":["# read in classification dataset and store in dataframe\n","df = pd.read_csv('/content/gdrive/MyDrive/data/eval_data/masked_tokens/chemistry/chemistry.tsv', sep=' ', index_col=0)"],"metadata":{"id":"UObwRG2FVe7g","executionInfo":{"status":"ok","timestamp":1650608702803,"user_tz":300,"elapsed":1131,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["df.head(n=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"ipTxGTOvVtAA","executionInfo":{"status":"ok","timestamp":1650608702803,"user_tz":300,"elapsed":11,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}},"outputId":"0a16054e-34a4-42de-865e-a412ec2c721a"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     masked_token                                        masked_line  \\\n","0       different  studies to evaluate donor substrates that offe...   \n","1     dissolution  we have found that the presence of napl and th...   \n","2            napl  the composition of pce [MSK] might be signific...   \n","3       microbial  it is important to gain a general understandin...   \n","4          actual  with a good numerical model the responses of s...   \n","5       reactions  one difficulty in modeling the biological [MSK...   \n","6          dnapls  because our goal is to stimulate a high pce tr...   \n","7        limiting  one critical [MSK] factor for enhanced dnapl d...   \n","8  dehalogenation  experimental studies to determine the nature a...   \n","9       evolution  directed [MSK] experiments with dhla are curre...   \n","\n","   masked_idx  \n","0           7  \n","1          26  \n","2           4  \n","3          14  \n","4          34  \n","5           6  \n","6          13  \n","7           2  \n","8          13  \n","9           1  "],"text/html":["\n","  <div id=\"df-780901fd-b633-41f8-a6f0-17b13bc41e56\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>masked_token</th>\n","      <th>masked_line</th>\n","      <th>masked_idx</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>different</td>\n","      <td>studies to evaluate donor substrates that offe...</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>dissolution</td>\n","      <td>we have found that the presence of napl and th...</td>\n","      <td>26</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>napl</td>\n","      <td>the composition of pce [MSK] might be signific...</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>microbial</td>\n","      <td>it is important to gain a general understandin...</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>actual</td>\n","      <td>with a good numerical model the responses of s...</td>\n","      <td>34</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>reactions</td>\n","      <td>one difficulty in modeling the biological [MSK...</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>dnapls</td>\n","      <td>because our goal is to stimulate a high pce tr...</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>limiting</td>\n","      <td>one critical [MSK] factor for enhanced dnapl d...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>dehalogenation</td>\n","      <td>experimental studies to determine the nature a...</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>evolution</td>\n","      <td>directed [MSK] experiments with dhla are curre...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-780901fd-b633-41f8-a6f0-17b13bc41e56')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-780901fd-b633-41f8-a6f0-17b13bc41e56 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-780901fd-b633-41f8-a6f0-17b13bc41e56');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["train_percent = 0.2\n","validation_percent = 0.05\n","test_percent = 0.05\n","\n","df = df.sample(frac=1)\n","a = int(len(df)*train_percent)\n","b = int(a + len(df)*validation_percent)\n","c = int(b + len(df)*test_percent)\n","\n","train_data = df[0:a]\n","validation_data = df[a:b]\n","test_data = df[b:c]\n","\n","print(train_data.head())\n","print(validation_data.head())\n","print(test_data.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"INFOtQiCVvFX","executionInfo":{"status":"ok","timestamp":1650608702804,"user_tz":300,"elapsed":10,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}},"outputId":"0ea5b979-9ff8-4b16-a3b8-ce38be19dcb4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["       masked_token                                        masked_line  \\\n","224933      protein  the atpase activity of reca p67w [MSK] was mea...   \n","209649    molecular  all atom [MSK] dynamics md simulations are cap...   \n","155749     anatomic  we evaluated the biodistribution of 4 in vivo ...   \n","181333          tbs  on each day of the culture a portion of the ce...   \n","221798   nitrifying  we recognize that using immunofluorescence to ...   \n","\n","        masked_idx  \n","224933           6  \n","209649           2  \n","155749          14  \n","181333          23  \n","221798           7  \n","       masked_token                                        masked_line  \\\n","8173         bonded  the components of the cocrystals form 1d hydro...   \n","148581   industries  mild steel is widely used as the constructiona...   \n","50048         cells  ganglion [MSK] schwann cells and clusters of a...   \n","87440      specific  protein samples were analysed by western blot ...   \n","175835         work  the differences de el de th dpv [MSK] term and...   \n","\n","        masked_idx  \n","8173             8  \n","148581          14  \n","50048            1  \n","87440            9  \n","175835           7  \n","       masked_token                                        masked_line  \\\n","91510   correlation  pearson [MSK] analysis between biological metr...   \n","68208         loose  in both cases we apply [MSK] constraints on al...   \n","152037    sectional  10 9 cm 2 from cross [MSK] tem images using th...   \n","225752      exhibit  such fluids [MSK] non newtonian characteristic...   \n","86500   crystalline  detailed study of the dependence between the t...   \n","\n","        masked_idx  \n","91510            1  \n","68208            5  \n","152037           6  \n","225752           2  \n","86500           10  \n"]}]},{"cell_type":"code","source":["class SentenceExample:\n","    \"\"\"\n","    Data wrapper for a single example for sentiment analysis.\n","\n","    Attributes:\n","        words (List[string]): list of words\n","        label (int): 0 or 1 (0 = negative, 1 = positive)\n","    \"\"\"\n","\n","    def __init__(self, words, masked_token, masked_idx):\n","        self.words = words\n","        self.masked_token = masked_token\n","        self.masked_idx = masked_idx\n","\n","    def __repr__(self):\n","        return repr(self.words) + \"; masked token =\" + repr(self.masked_token)\n","\n","    def __str__(self):\n","        return self.__repr__()"],"metadata":{"id":"wV7x4_D9V40J","executionInfo":{"status":"ok","timestamp":1650608702804,"user_tz":300,"elapsed":7,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class Indexer(object):\n","    \"\"\"\n","    Bijection between objects and integers starting at 0. Useful for mapping\n","    labels, features, etc. into coordinates of a vector space.\n","\n","    Attributes:\n","        objs_to_ints\n","        ints_to_objs\n","    \"\"\"\n","    def __init__(self):\n","        self.objs_to_ints = {}\n","        self.ints_to_objs = {}\n","\n","    def __repr__(self):\n","        return str([str(self.get_object(i)) for i in range(0, len(self))])\n","\n","    def __str__(self):\n","        return self.__repr__()\n","\n","    def __len__(self):\n","        return len(self.objs_to_ints)\n","\n","    def get_object(self, index):\n","        \"\"\"\n","        :param index: integer index to look up\n","        :return: Returns the object corresponding to the particular index or None if not found\n","        \"\"\"\n","        if (index not in self.ints_to_objs):\n","            return None\n","        else:\n","            return self.ints_to_objs[index]\n","\n","    def contains(self, object):\n","        \"\"\"\n","        :param object: object to look up\n","        :return: Returns True if it is in the Indexer, False otherwise\n","        \"\"\"\n","        return self.index_of(object) != -1\n","\n","    def index_of(self, object):\n","        \"\"\"\n","        :param object: object to look up\n","        :return: Returns -1 if the object isn't present, index otherwise\n","        \"\"\"\n","        if (object not in self.objs_to_ints):\n","            return -1\n","        else:\n","            return self.objs_to_ints[object]\n","\n","    def add_and_get_index(self, object, add=True):\n","        \"\"\"\n","        Adds the object to the index if it isn't present, always returns a nonnegative index\n","        :param object: object to look up or add\n","        :param add: True by default, False if we shouldn't add the object. If False, equivalent to index_of.\n","        :return: The index of the object\n","        \"\"\"\n","        if not add:\n","            return self.index_of(object)\n","        if (object not in self.objs_to_ints):\n","            new_idx = len(self.objs_to_ints)\n","            self.objs_to_ints[object] = new_idx\n","            self.ints_to_objs[new_idx] = object\n","        return self.objs_to_ints[object]"],"metadata":{"id":"0NqMEh1SeISP","executionInfo":{"status":"ok","timestamp":1650608702804,"user_tz":300,"elapsed":7,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class WordEmbeddings:\n","    \"\"\"\n","    Wraps an Indexer and a list of 1-D numpy arrays where each position in the list is the vector for the corresponding\n","    word in the indexer. The 0 vector is returned if an unknown word is queried.\n","    \"\"\"\n","    def __init__(self, word_indexer, vectors):\n","        self.word_indexer = word_indexer\n","        self.vectors = vectors\n","\n","    def get_embedding_length(self):\n","        return len(self.vectors[0])\n","\n","    def get_embedding(self, word):\n","        \"\"\"\n","        Returns the embedding for a given word\n","        :param word: The word to look up\n","        :return: The UNK vector if the word is not in the Indexer or the vector otherwise\n","        \"\"\"\n","        word_idx = self.word_indexer.index_of(word)\n","        if word_idx != -1:\n","            return self.vectors[word_idx]\n","        else:\n","            return self.vectors[self.word_indexer.index_of(\"UNK\")]"],"metadata":{"id":"B4cfDfczeLGC","executionInfo":{"status":"ok","timestamp":1650608702805,"user_tz":300,"elapsed":7,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def read_word_embeddings(embeddings_file: str) -> WordEmbeddings:\n","    \"\"\"\n","    Loads the given embeddings (ASCII-formatted) into a WordEmbeddings object. Augments this with an UNK embedding\n","    that is the 0 vector. Reads in all embeddings with no filtering -- you should only use this for relativized\n","    word embedding files.\n","    :param embeddings_file: path to the file containing embeddings\n","    :return: WordEmbeddings object reflecting the words and their embeddings\n","    \"\"\"\n","    f = open(embeddings_file)\n","    word_indexer = Indexer()\n","    vectors = []\n","    # Make position 0 a PAD token, which can be useful if you\n","    word_indexer.add_and_get_index(\"PAD\")\n","    # Make position 1 the UNK token\n","    word_indexer.add_and_get_index(\"UNK\")\n","    for i, line in enumerate(f):\n","        if line.strip() != \"\":\n","            space_idx = line.find(' ')\n","            word = line[:space_idx]\n","            numbers = line[space_idx+1:]\n","            float_numbers = [float(number_str) for number_str in numbers.split()]\n","            vector = np.array(float_numbers)\n","            word_indexer.add_and_get_index(word)\n","            # Append the PAD and UNK vectors to start. Have to do this weirdly because we need to read the first line\n","            # of the file to see what the embedding dim is\n","            if len(vectors) == 0:\n","                vectors.append(np.zeros(vector.shape[0]))\n","                vectors.append(np.zeros(vector.shape[0]))\n","            vectors.append(vector)\n","        if i % 10000 == 0:\n","          print(f'done reading {i} embeddings')\n","    f.close()\n","    print(\"Read in \" + repr(len(word_indexer)) + \" vectors of size \" + repr(vectors[0].shape[0]))\n","    # Turn vectors into a 2-D numpy array\n","    return WordEmbeddings(word_indexer, np.array(vectors))"],"metadata":{"id":"AfKpwulaeNID","executionInfo":{"status":"ok","timestamp":1650608702805,"user_tz":300,"elapsed":7,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["embeddings = read_word_embeddings(\"/content/gdrive/MyDrive/embeddings/glove/baseline/glove.6B.300d.txt\")\n","print(len(embeddings.word_indexer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2RZR3fJkeSvS","executionInfo":{"status":"ok","timestamp":1650608766035,"user_tz":300,"elapsed":63237,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}},"outputId":"7cf2dfae-1b50-4efc-eb22-cfd65d239727"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["done reading 0 embeddings\n","done reading 10000 embeddings\n","done reading 20000 embeddings\n","done reading 30000 embeddings\n","done reading 40000 embeddings\n","done reading 50000 embeddings\n","done reading 60000 embeddings\n","done reading 70000 embeddings\n","done reading 80000 embeddings\n","done reading 90000 embeddings\n","done reading 100000 embeddings\n","done reading 110000 embeddings\n","done reading 120000 embeddings\n","done reading 130000 embeddings\n","done reading 140000 embeddings\n","done reading 150000 embeddings\n","done reading 160000 embeddings\n","done reading 170000 embeddings\n","done reading 180000 embeddings\n","done reading 190000 embeddings\n","done reading 200000 embeddings\n","done reading 210000 embeddings\n","done reading 220000 embeddings\n","done reading 230000 embeddings\n","done reading 240000 embeddings\n","done reading 250000 embeddings\n","done reading 260000 embeddings\n","done reading 270000 embeddings\n","done reading 280000 embeddings\n","done reading 290000 embeddings\n","done reading 300000 embeddings\n","done reading 310000 embeddings\n","done reading 320000 embeddings\n","done reading 330000 embeddings\n","done reading 340000 embeddings\n","done reading 350000 embeddings\n","done reading 360000 embeddings\n","done reading 370000 embeddings\n","done reading 380000 embeddings\n","done reading 390000 embeddings\n","Read in 400002 vectors of size 300\n","400002\n"]}]},{"cell_type":"code","source":["print(embeddings.word_indexer.index_of('particles'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fFYVWMlAeUiC","executionInfo":{"status":"ok","timestamp":1650608766035,"user_tz":300,"elapsed":4,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}},"outputId":"f1ce27c7-b519-4532-ee81-e1b257ef38ae"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["9100\n"]}]},{"cell_type":"code","source":["train_exs = []\n","for i, row in train_data.iterrows():\n","  sentence = row['masked_line'].split(' ')\n","  sentence[-1] = sentence[-1].strip()\n","  train_exs.append(SentenceExample(sentence, row['masked_token'], row['masked_idx']))"],"metadata":{"id":"ny-DFM7HdfHj","executionInfo":{"status":"ok","timestamp":1650608770570,"user_tz":300,"elapsed":4537,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["validation_exs = []\n","for i, row in validation_data.iterrows():\n","  sentence = row['masked_line'].split(' ')\n","  sentence[-1] = sentence[-1].strip()\n","  validation_exs.append(SentenceExample(sentence, row['masked_token'], row['masked_idx']))"],"metadata":{"id":"X9ngdsZKd3UV","executionInfo":{"status":"ok","timestamp":1650608771873,"user_tz":300,"elapsed":1305,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["test_exs = []\n","for i, row in test_data.iterrows():\n","  sentence = row['masked_line'].split(' ')\n","  sentence[-1] = sentence[-1].strip()\n","  test_exs.append(SentenceExample(sentence, row['masked_token'], row['masked_idx']))"],"metadata":{"id":"klDoFPGVd-4c","executionInfo":{"status":"ok","timestamp":1650608773080,"user_tz":300,"elapsed":1210,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["class MaskedWordPredictor(nn.Module):\n","  def __init__(self, \n","               embedding_vectors,\n","               hidden_dim,\n","               output_dim):\n","    super().__init__()\n","\n","    self.embedding_vectors = embedding_vectors\n","    self.hidden_dim = hidden_dim\n","    self.output_dim = output_dim\n","\n","    weights = torch.FloatTensor(self.embedding_vectors)\n","    self.e1 = nn.Embedding.from_pretrained(weights, padding_idx=0)\n","    self.e1.requires_grad_(False)\n","\n","    self.lstm = nn.LSTM(input_size=len(self.embedding_vectors[0]),\n","                        hidden_size=self.hidden_dim,\n","                        num_layers=2,\n","                        batch_first=True,\n","                        bidirectional=True)\n","    \n","    self.drop = nn.Dropout(p=0.3)\n","    self.fc1 = nn.Linear(2*self.hidden_dim, output_dim)\n","\n","  def forward(self, x, masked_idxs):\n","\n","    sentence_mask = (x != 0).type(\n","            torch.cuda.LongTensor if x.is_cuda else\n","            torch.LongTensor)\n","    \n","    sentence_lengths = sentence_mask.sum(dim=1).cpu()\n","            \n","    x = self.e1(x)\n","\n","    packed_input = pack_padded_sequence(x, sentence_lengths, batch_first=True, enforce_sorted=False)\n","    packed_output, _ = self.lstm(packed_input)\n","    output, _ = pad_packed_sequence(packed_output, batch_first=True)    \n","\n","    masked_embeddings = torch.cat([torch.index_select(a, 0, i) for a, i in zip(output, masked_idxs)])\n","\n","    x = self.fc1(masked_embeddings)\n","\n","    return x"],"metadata":{"id":"Wjo8uQ69e8MC","executionInfo":{"status":"ok","timestamp":1650608773080,"user_tz":300,"elapsed":4,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["class RNNClassifier():\n","    \"\"\"\n","    Implement your NeuralSentimentClassifier here. This should wrap an instance of the network with learned weights\n","    along with everything needed to run it on new data (word embeddings, etc.)\n","    \"\"\"\n","    def __init__(self,\n","                 word_embeddings,\n","                 output_size,\n","                 batch_size=128,\n","                 hidden_size=128,\n","                 lr=0.001,\n","                 num_epochs=10,\n","                 seed=3):\n","\n","        # indexer between words and indexes => self.word_embeddings.word_indexer (indexer in utils)\n","        # 2D array of weights => self.word_embeddings.vectors (in sentiment_data)\n","        \n","        self.word_embeddings = word_embeddings\n","        self.batch_size = batch_size\n","        self.num_epochs = num_epochs\n","        self.lr = lr\n","        self.output_size = output_size\n","\n","        # random.seed(seed)\n","        # torch.manual_seed(seed)\n","\n","        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","        self.network = MaskedWordPredictor(embedding_vectors=self.word_embeddings.vectors,\n","                                           hidden_dim=hidden_size,\n","                                           output_dim=output_size).to(device)\n","\n","        self.print_network()\n","\n","        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","    def convert_to_idx_tensor(self, sentences):\n","        idx_encodings = []\n","        for sentence in sentences:\n","            idx_encoding = [0] * len(sentence)\n","            for idx, word in enumerate(sentence):\n","                word_idx = self.word_embeddings.word_indexer.index_of(word)\n","                idx_encoding[idx] = word_idx if word_idx != -1 else 1\n","            idx_encodings.append(idx_encoding)\n","        idx_encodings = torch.tensor(idx_encodings)\n","        return idx_encodings\n","\n","    def pad_sentences(self, batch):\n","        max_len = 0\n","        for sentiment_ex in batch:\n","            sentence = sentiment_ex.words\n","            max_len = max(max_len, len(sentence))\n","\n","        for sentiment_ex in batch:\n","            sentence = sentiment_ex.words\n","            sentence += ['PAD'] * (max_len - len(sentence))\n","\n","        return batch\n","\n","    def train(self, train_exs, dev_exs):\n","        \n","        loss_function = nn.CrossEntropyLoss()\n","        optimizer = optim.Adam(self.network.parameters(), lr=self.lr, weight_decay=0.0001)\n","        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.95)\n","\n","        max_dev_acc = 0\n","        best_weights = None\n","\n","        for epoch in range(self.num_epochs):\n","\n","            self.network.train()\n","            random.shuffle(train_exs)\n","\n","            \"\"\"\n","            Minibatch stuff\n","            \"\"\"\n","            idx = 0\n","            minibatch_train_exs = []\n","            for i in range(0, len(train_exs), self.batch_size):\n","                minibatch_train_exs.append(train_exs[i:i+self.batch_size])\n","                self.pad_sentences(minibatch_train_exs[idx])\n","                idx += 1\n","\n","            total_loss = 0\n","            for i, exs in enumerate(minibatch_train_exs):\n","\n","                train_x = [ex.words for ex in exs]\n","                train_y = [max(self.word_embeddings.word_indexer.index_of(ex.masked_token), 1) for ex in exs]\n","                train_masked_idxs = torch.tensor([ex.masked_idx for ex in exs]).to(self.device)\n","\n","                # convert word sentences to list of indexes and train_y to tensor\n","                x = self.convert_to_idx_tensor(train_x)\n","                y = torch.tensor(train_y)\n","\n","                x = x.to(self.device)\n","                y = y.to(self.device)\n","\n","                output = self.network(x, train_masked_idxs)\n","\n","                loss = loss_function(output, y)\n","                total_loss += loss\n","    \n","                # update model weights\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","                print(f'batch {i} complete')\n","\n","            # calculate validation set accuracy\n","            golds = [max(self.word_embeddings.word_indexer.index_of(dev_ex.masked_token), 1) for dev_ex in dev_exs]\n","            predictions = self.predict_all([dev_ex.words for dev_ex in dev_exs], \n","                                            torch.tensor([dev_ex.masked_idx for dev_ex in dev_exs]).to(self.device))\n","            num_correct = 0\n","            for i in range(0, len(golds)):\n","                if golds[i] == predictions[i]:\n","                    num_correct += 1\n","\n","            dev_acc = num_correct/len(golds)\n","\n","            if dev_acc > max_dev_acc or epoch == 0:\n","                max_dev_acc = dev_acc\n","                best_weights = deepcopy(self.network.state_dict())\n","                print('saving model weights...')\n","\n","            scheduler.step()\n","\n","            print(f'Epoch Number = {epoch}, total loss = ', total_loss)\n","            print(f'Development Accuracy = {num_correct}/{len(golds)} = {dev_acc}')\n","\n","        self.network.load_state_dict(best_weights)\n","\n","    def predict(self, ex_words, masked_idx) -> int:\n","        idx_tensor = self.convert_to_idx_tensor([ex_words])\n","        idx_tensor = idx_tensor.to(self.device)\n","        output = self.network(idx_tensor, masked_idx)\n","        result = output.argmax(dim=1)\n","        return result[0]\n","\n","    def predict_all(self, all_ex_words, dev_masked_idxs):\n","        \"\"\"\n","        You can leave this method with its default implementation, or you can override it to a batched version of\n","        prediction if you'd like. Since testing only happens once, this is less critical to optimize than training\n","        for the purposes of this assignment.\n","        :param all_ex_words: A list of all exs to do prediction on\n","        :return:\n","        \"\"\"\n","        return [self.predict(ex_words, [masked_idx]) for ex_words, masked_idx in zip(all_ex_words, dev_masked_idxs)]\n","\n","    def print_network(self):\n","        print(self.network)"],"metadata":{"id":"2oKfqO5ifkVh","executionInfo":{"status":"ok","timestamp":1650608773204,"user_tz":300,"elapsed":127,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def train_deep_averaging_network(train_exs, \n","                                 dev_exs,\n","                                 word_embeddings: WordEmbeddings) -> RNNClassifier:\n","    \"\"\"\n","    :param args: Command-line args so you can access them here\n","    :param train_exs: training examples\n","    :param dev_exs: development set, in case you wish to evaluate your model during training\n","    :param word_embeddings: set of loaded word embeddings\n","    :return: A trained NeuralSentimentClassifier model\n","    \"\"\"\n","\n","    # extract input information from args\n","    batch_size = 128 # args.batch_size\n","    hidden_size = 128 # args.hidden_size\n","    lr = 0.001 # args.lr\n","    num_epochs = 10 # args.num_epochs\n","    output_size = len(word_embeddings.vectors)\n","\n","    classifier = RNNClassifier(word_embeddings=word_embeddings,\n","                               output_size=output_size,\n","                               batch_size=batch_size, \n","                               hidden_size=hidden_size, \n","                               lr=lr, \n","                               num_epochs=num_epochs)\n","\n","    start_time = time.time()\n","    classifier.train(train_exs=train_exs, dev_exs=dev_exs)\n","    end_time = time.time()\n","    total_time = int(end_time - start_time)\n","\n","    print(f'Total time taken => {total_time}s')\n","\n","    return classifier"],"metadata":{"id":"bhYfPBoegnnK","executionInfo":{"status":"ok","timestamp":1650608773293,"user_tz":300,"elapsed":3,"user":{"displayName":"NLP Final","userId":"00834871271301784662"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["model = train_deep_averaging_network(train_exs, validation_exs, embeddings)"],"metadata":{"id":"_65O1Yhsg-iF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" "],"metadata":{"id":"OiftyokKhAHS"},"execution_count":null,"outputs":[]}]}